{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./data/sentiment labelled sentences/amazon_cells_labelled.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So there is no way f'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(text) # Vocabulary, the set of characters in our text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map between characters and integer indices\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char2idx['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2char[72]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and target examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainig parameters\n",
    "seq_length = 50\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# We need to encode the full text list into integers\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "o\n",
      " \n",
      "t\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the batch method of char_dataset to generate training batches\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  So there is no way for me to plug it in here in th\n",
      "Output:  o there is no way for me to plug it in here in the\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input: \", ''.join(idx2char[input_example.numpy()]))\n",
    "    print(\"Output: \", ''.join(idx2char[target_example.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = dataset.shuffle(5000).batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024 # Recurrent neural network units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: We need two different modes here, inference and training, and to build the model in two different ways.\n",
    "\n",
    "- For training, we want our model to take batches of several (input_example, target_example) pairs.\n",
    "- For inference, we want to start with a single seed string, and get it writing from that point on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_size=batch_size), # Specify batch dimensions and embedding (reshape)\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences = True,\n",
    "                            stateful = True), \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for input_example, target_example in final_dataset.take(1):\n",
    "#    example_batch_preds = model(input_example)\n",
    "#    print(example_batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, probas):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, probas, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 22s 1s/step - loss: 4.2791\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 3.4196\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 2.9455\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 2.6201\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 2.4302\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 2.3359\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 2.2692\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 31s 2s/step - loss: 2.2150\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 2.1650\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 31s 2s/step - loss: 2.1187\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 31s 2s/step - loss: 2.0636\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 2.0078\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 1.9591\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.9074\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.8554\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.7991\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.7537\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.7058\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 1.6554\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.6139\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.5706\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 1.5342\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 1.4807\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.4408\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.4006\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.3558\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.3138\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.2662\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.2263\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.1840\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.1353\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.0941\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.0418\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9945\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9502\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9100\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.8650\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.8102\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.7643\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.7233\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.6858\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.6395\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.6073\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.5768\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.5430\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.5221\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.4956\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.4688\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.4496\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.4272\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.4096\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.3977\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.3843\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3754\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3555\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.3540\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3440\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3316\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3240\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3211\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3133\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3098\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2940\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.3001\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2941\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2873\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2783\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.2852\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.2718\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.2700\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.2681\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.2632\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.2569\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.2550\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.2593\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.2487\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.2481\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2414\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.2463\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2450\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 41s 2s/step - loss: 0.2361\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 31s 2s/step - loss: 0.2396\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 30s 2s/step - loss: 0.2368\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2335\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2334\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 32s 2s/step - loss: 0.2323\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.2296\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.2225\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 35s 2s/step - loss: 0.2289\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.2260\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.2217\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.2221\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.2176\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.2202\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.2183\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.2162\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 30s 2s/step - loss: 0.2122\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.2121\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.2104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aa1e30cb08>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(final_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Check what our model is doing. start_string will be a text string\n",
    "    \n",
    "    num_generate = 1000 # number of characters that the model will generate\n",
    "    \n",
    "    # First, we vectorize our start string\n",
    "    input_vec = [char2idx[s] for s in start_string]\n",
    "    input_vec = tf.expand_dims(input_vec, 0) # make dimension consistent with tf\n",
    "    \n",
    "    text_generated = [] # here goes the generated text\n",
    "    \n",
    "    temperature = 1.0 # High values will yield more surprising text :)\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_vec)\n",
    "        predictions = tf.squeeze(predictions, 0) # removes the batch dimension\n",
    "        \n",
    "        # Rescale by temp, and make a draw for the next-char id's\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # Finally, pass this id to idx2char\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "        # Add the new character and pass to the next step in the loop\n",
    "        input_vec = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought this product is great.\\t1\\nI have to jightly and is sturdy on the market yet.\\t1\\nSWEETEST PHONE!!!\\t1\\n:-) in unyone!.\\t1\\nI wouldn\\'t make the same mistake I did.\\t0\\nOh, the better run out favadit on and the sound is much better than otherso use because case.\\t0\\nIt\\'s AGGRAVATI got weak snap!\\t0\\nDoes not charge the Cingular (ATT) 853 my calls and carries the highest quality of anti-glare se with no problem with this item and would order it again if needed.\\t1\\nYou get what you pay for.\\t0\\nThe handsfree part what you pay for product.\\t0\\nHow can that be?The audio quality and weil better funnt, the menus and dirction feels good.\\t1\\nI received my headset in good time and with no stars because it toucher contacting the company for few dollar product is very Highfand noticed than inside.\\t1\\nA lot of websites have been satisife.\\t0\\nIt\\'s not what it says it is.\\t0\\nBut now that it is \"outyour laptou plan to use this in a car forgets super charged up for use as a small hybrid palmtone as a long to mess with my new Palm VX and '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, start_string='I bought this product ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
